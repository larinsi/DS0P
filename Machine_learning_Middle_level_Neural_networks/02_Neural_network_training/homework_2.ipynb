{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRRejAlppFwP"
   },
   "source": [
    "# Обучение нейронной сети\n",
    "\n",
    "Это последняя и самая важная практика в этом блоке. В ней вы соберете воедино все, что мы с вами изучили и примените для создания сети, которая классифицирует рукописные цифры.\n",
    "\n",
    "Задание будет состоять из следующих этапов:\n",
    "1.  Реализация слоя ReLU\n",
    "2.  Реализация полносвязного слоя\n",
    "3.  Написание обучающего цикла\n",
    "4.  Загрузка данных и обучение сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZpTEvkPqxqk"
   },
   "source": [
    "**Задание 1**\n",
    "Написание слоя ReLU. В этом задании вы реализуете слой нейронной сети, который вычисляет поэлементно функцию ReLU:\n",
    "\n",
    "$$\n",
    "ReLU(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "и выгядит следующим образом:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/536/1*oePAhrm74RNnNEolprmTaQ.png)\n",
    "\n",
    "Нетрудно заметить, что производная при $x > 0$ равна $1$, а при $x < 0$ равна $0$. Это вам пригодится при реализации backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XKoIb3NpLAqj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import derivative\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "7fiEdDgzLFiH",
    "outputId": "9ce90d56-4f98-4392-fbb3-96edb9d50071"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Базовый класс слоя нашей нейронной сети. \n",
    "    Все слои должны наследоваться от него и реализовывать два метода: forward и backward\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    def backward(self, dL_dz, learning_rate=0):\n",
    "        pass\n",
    "\n",
    "class ReLU(Layer):\n",
    "    \"\"\"\n",
    "    Слой ReLU\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Метод, который вычисляет ReLU(x)\n",
    "\n",
    "        Размер выхода должен совпадать со входом\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self._saved_input = x # нам необходимо сохранить вход\n",
    "        output = None\n",
    "\n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # переменная output должна содержать выход ReLU\n",
    "\n",
    "        # подсказка: метод clip библиотеки numpy позволяет при заданном интервале значения вне интервала обрезать по краям интервала. \n",
    "        # Например, если указан интервал [0, 1], \n",
    "        # значения меньше 0 становятся 0, а значения больше 1 становятся 1.\n",
    "        \n",
    "        output = np.clip(x, a_min=0, a_max=None)\n",
    "        \n",
    "        # < YOUR CODE ENDS HERE >\n",
    "        assert output.shape == x.shape\n",
    "        return output\n",
    "\n",
    "    def backward(self, dL_dz, learning_rate=0.):\n",
    "        \"\"\"\n",
    "        dL_dz -- производная финальной функции по выходу этого слоя.\n",
    "                 Размерость должна в точности соответствовать размерности\n",
    "                 x, который прошел в forward pass.\n",
    "        learning_rate -- не используется, т.к. ReLU не содержит параметров.\n",
    "\n",
    "        Метод должен посчитать производную dL_dx.\n",
    "        Благодаря chain rule, мы знаем, что dL_dx = dL_dz * dz_dx\n",
    "        и при этом dL_dz нам известна.\n",
    "\n",
    "        Для слоя relu, dz_dx(x) = 1, при x > 0, и dz_dz = 0 при x < 0\n",
    "        \n",
    "        * сохраненный инпут находится в self._saved_input\n",
    "        \"\"\"\n",
    "        dz_dx = None\n",
    "        \n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # переменная dz_dx должна содержать производную выхода ReLU по ее входу\n",
    "        \n",
    "        dz_dx = np.where(self._saved_input<0, 0, 1)\n",
    "        \n",
    "        # < YOUR CODE ENDS HERE >\n",
    "        assert dz_dx.shape == self._saved_input.shape, f\"Shapes must be the same. Got {dz_dx.shape, self._saved_input.shape}\"\n",
    "        output = dz_dx * dL_dz\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OWFBXJGPz_zt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x252cc8b5090>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "\n",
    "# убедитесь, что график соответствует представленному вверху\n",
    "plt.plot(np.linspace(-1, 1, 100), relu.forward(np.linspace(-1, 1, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aH8pmNuI3BBP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: ReLU().forward(x)\n",
    "\n",
    "x = np.linspace(-1, 1, 10*32).reshape([10, 32])\n",
    "l = ReLU()\n",
    "l.forward(x)\n",
    "grads = l.backward(np.ones([10, 32]))\n",
    "numeric_grads = derivative(f, x, dx=1e-6)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "     \"gradient returned by your layer does not match the numerically computed gradient\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddzfoKcogJfE"
   },
   "source": [
    "**Задание 2** Реализация полносвязного слоя. \n",
    "\n",
    "Закончите реализацию метода forward для полносвязного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5DQKmPuqKUp"
   },
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    \"\"\"\n",
    "    Полносвязный (fully connected/dense) слой\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        in_dim, out_dim -- количество входных и выходных нейронов соответственно\n",
    "        \"\"\"\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        # инициализируем матрицу весов (in_dim,out_dim) нормальным распределением\n",
    "        self.weights = np.random.randn(in_dim,out_dim)*0.001\n",
    "\n",
    "        # инициализируем смещение нулями\n",
    "        self.bias = np.zeros(self.out_dim)\n",
    "        self._saved_input = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление выхода полносвязного слоя.\n",
    "\n",
    "        x -- вход слоя, размерности (N, in_dim), где N -- количество объектов \n",
    "             в батче\n",
    "\n",
    "        return: matmul(x, weights) + bias\n",
    "        \"\"\"\n",
    "        assert np.ndim(x) == 2\n",
    "        assert x.shape[1] == self.in_dim\n",
    "        self._saved_input = x\n",
    "\n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # переменная output должна содержать выход полносвязного слоя\n",
    "        \n",
    "        output = \n",
    "        \n",
    "        # < YOUR CODE ENDS HERE >\n",
    "\n",
    "        assert output.shape == (x.shape[0], self.out_dim), (output.shape, (x.shape[0], self.out_dim))\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dL_dz, learning_rate=0.):\n",
    "        \"\"\"\n",
    "        dL_dz -- производная финальной функции по выходу этого слоя.\n",
    "                 Размерость (N, self.out_dim).\n",
    "        learning_rate -- если отличен от нуля, то с вызовом этой функции, параметры\n",
    "                         слоя (weights, bias) будут обновлены\n",
    "\n",
    "        Метод должен посчитать производную dL_dx.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert np.ndim(dL_dz) == 2\n",
    "        assert dL_dz.shape[1] == self.out_dim\n",
    "        \n",
    "        # очень рекомендуем понять почему это так,\n",
    "        # хорошее объяснение здесь: http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
    "        self.dL_dw = np.dot(self._saved_input.T, dL_dz)\n",
    "        self.dL_dx = np.dot(dL_dz, self.weights.T)\n",
    "        self.dL_db = dL_dz.sum(0) \n",
    "        \n",
    "        assert self.dL_db.shape == self.bias.shape\n",
    "        assert self.dL_dw.shape == self.weights.shape\n",
    "        assert self.dL_dx.shape == self._saved_input.shape\n",
    "\n",
    "        if learning_rate != 0:\n",
    "            # знакомый вам шаг градиентного спуска!\n",
    "            self.weights -= learning_rate * self.dL_dw\n",
    "            self.bias -= learning_rate * self.dL_db\n",
    "        \n",
    "        return self.dL_dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pREQ8CMW_Bcw"
   },
   "source": [
    "Теперь перейдем к вычислению лосса. Т.к. перед нами стоит задача мультиклассовой классификации, то мы будем использовать multiclass_cross_entropy_loss, который мы рассмотрели на лекции.\n",
    "Такой лосс на одном объекте $i$, который принадлежит к классу $k$ будет равен: \n",
    "$$\n",
    "loss_i = -log(p_i^k),\n",
    "$$\n",
    "где $p_i^k$ -- предсказанная вероятность того, что $i$ый объект принадлежит к классу $k$.\n",
    "\n",
    "Для того, чтобы получить вероятности, мы использовали Softmax, примененный в логитам выходного слоя. Т.е. \n",
    "$$\n",
    "p_i^k = \\frac{exp(logit_k)}{\\sum_{j=0}^{m} exp(logit_j)}\n",
    "$$\n",
    "\n",
    "А значит лосс можно переписать так:\n",
    "$$\n",
    "loss_i = -log(p_i^k) = -logit_k + log(\\sum_{j=0}^{m} exp(logit_j))\n",
    "$$\n",
    "\n",
    "Как мы помним, сеть изначально предсказывает логиты, и затем мы превращаем их в вероятности. Но т.к. мы знаем, что нам предстоит считать лосс, то мы можем не тратить \"силы\" на вычисление вероятностей и посчитать лосс основываясь на логитах. Такая запись проще и вычислительно более стабильная. \n",
    "\n",
    "Ниже мы приводим готовую реализацию лосса и его градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5b5T9VG1ohv"
   },
   "outputs": [],
   "source": [
    "def multiclass_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "    logits -- выход нейронной сети без активации. Размерность: (N, k),\n",
    "              где N -- количество объектов, k -- количество классов\n",
    "    y_true -- реальные классы для N объектов\n",
    "\n",
    "    Класс возвращает вектор из лоссов на каждом объекте\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    logits_for_answers = logits[np.arange(len(logits)), y_true]\n",
    "    \n",
    "    cross_entropy = -logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return cross_entropy\n",
    "\n",
    "def grad_multiclass_crossentropy_with_logits(logits, y_true):\n",
    "    \"\"\"\n",
    "     logits -- выход нейронной сети без активации. Размерность: (N, k),\n",
    "              где N -- количество объектов, k -- количество классов\n",
    "    y_true -- реальные классы для N объектов\n",
    "\n",
    "    Класс возвращает матрицу производных.\n",
    "    \n",
    "    \"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)), y_true] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH5c18IUgtj5"
   },
   "source": [
    "**Задание 3** Реализация класса  Network.\n",
    "\n",
    "В этом задании вам предлагается реализовать методы forward(), predict(), train_step(). Это последний шаг перед тем как наша сеть будет готова!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-H0nTyx1lTH"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    Нейронная сеть\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        \"\"\"\n",
    "        Для инициализации нейронной сети, нам нужен список слоев, которые должны\n",
    "        быть последовательно применены друг к другу. \n",
    "\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
    "        \n",
    "        Получив x на вход, сеть должна по-очереди применить к нему все слои.\n",
    "        Т.е. выход каждого слоя является входом следующего.\n",
    "\n",
    "        x -> layer_0 -> layer_1 ... -> last_layer\n",
    "\n",
    "        \"\"\"\n",
    "        output = None\n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # Реализуйте последовательное применение forward методов для каждого из \n",
    "        # слоев (self.layers)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # < YOUR CODE ENDS HERE >\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
    "        \n",
    "        Функция должна вернуть вектор размера (N) с номером предсказанного класса\n",
    "        для каждого объекта. \n",
    "\n",
    "        \"\"\"\n",
    "        logits = self.forward(x) # считаем логиты сделав полный форвард пасс сети\n",
    "        # напомним, что размер логитов (N, k), где k -- количество классов\n",
    "        \n",
    "        classes = None\n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # реализуйте получение классов из логитов. вам может пригодится \n",
    "        # метод argmax\n",
    "\n",
    "        classes = \n",
    "\n",
    "        # < YOUR CODE ENDS HERE >\n",
    "\n",
    "        assert classes.shape == (x.shape[0],)\n",
    "        return classes\n",
    "    \n",
    "    def train_step(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
    "        y -- реальные классы объектов (N,)\n",
    "        \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        loss = multiclass_crossentropy_with_logits(logits, y)\n",
    "        loss_grad = grad_multiclass_crossentropy_with_logits(logits, y)\n",
    "        \n",
    "        # < YOUR CODE STARTS HERE >\n",
    "        # Выше мы получили loss_grad. Теперь его нужно \"пробросить\" через всю сеть\n",
    "        # вызывая backward каждого слоя в обратном порядке.\n",
    "        # Не забудьте передать в backward -- learning rate.\n",
    "        # loss_grad -> last_layer.backward(loss_grad, lr) -> ... --> layer_0.backward(loss_grad_from_layer_1, lr)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # < YOUR CODE ENDS HERE >\n",
    "\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def fit(self, x_train, y_train, x_test, y_test, learning_rate, num_epochs, \n",
    "            batch_size):\n",
    "        \"\"\"\n",
    "        Цикл обучения уже реализован. Основная его задача -- итерироваться по \n",
    "        минибатчам и вызывать на каждом из них train_step, который вы уже реализовали.\n",
    "\n",
    "        В остальном -- это логирование лосса, точности и отрисовка графика.\n",
    "\n",
    "        \"\"\"\n",
    "        train_log = []\n",
    "        test_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_iters = []\n",
    "            for x_batch,y_batch in iterate_minibatches(x_train, y_train, \n",
    "                                                       batchsize=batch_size, shuffle=True):\n",
    "                loss_iters.append(self.train_step(x_batch, y_batch, learning_rate=learning_rate))\n",
    "\n",
    "            loss_log.append(np.mean(loss_iters)) # для визуализации усредняем лосс за каждую итерацию\n",
    "            train_accuracy = accuracy_score(y_train, self.predict(x_train))\n",
    "            test_accuracy = accuracy_score(y_test, self.predict(x_test))\n",
    "            train_log.append(train_accuracy)\n",
    "            test_log.append(test_accuracy)\n",
    "\n",
    "            clear_output()\n",
    "            print(\"Epoch\", epoch)\n",
    "            print(\"Train accuracy:\",train_log[-1])\n",
    "            print(\"Test accuracy:\",test_log[-1])\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            ax1 = plt.subplot(1,2,1)    \n",
    "            plt.plot(train_log,label='train accuracy')\n",
    "            plt.plot(test_log,label='test accuracy')\n",
    "            ax2 = plt.subplot(1,2,2)\n",
    "            plt.plot(loss_log,label='loss')\n",
    "            ax1.legend(loc='best')\n",
    "            ax2.legend(loc='best')\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            batch_indexes = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            batch_indexes = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "        yield inputs[batch_indexes], targets[batch_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QirWyEeEPS3w"
   },
   "source": [
    "## Обучение\n",
    "На этом этапе наша сеть полностью описаны и готова к бою. Нам нужны теперь только данные.\n",
    "\n",
    "Мы поставим перед собой задачу классифицировать изображения рукописных цифр. Они представляют собой картинки размера (28, 28). Для использования нашей сети мы превратим их в строчки длины $28*28 = 784$.\n",
    "\n",
    "Давайте посмотрим как они выглядят:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHeWIyY4fER3"
   },
   "outputs": [],
   "source": [
    "def show_mnist(images, labels, predicted_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4,4, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "        if predicted_labels is not None:\n",
    "            title_obj = plt.title(f\"Real: {labels[i]}. Pred: {predicted_labels[i]}\")\n",
    "            if labels[i] != predicted_labels[i]:\n",
    "                plt.setp(title_obj, color='r')\n",
    "        else:\n",
    "            plt.title(f\"Real label: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prp9Oo0mhNbf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "show_mnist(train_images, train_labels)\n",
    "\n",
    "# преобразуем изображения к нужному виду\n",
    "train_images = train_images.reshape(train_images.shape[0], -1).astype('float32') / 255.\n",
    "test_images = test_images.reshape(test_images.shape[0], -1).astype('float32') / 255.\n",
    "print(train_images.shape, test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFTgCXa4QQZm"
   },
   "outputs": [],
   "source": [
    "# Наша сеть будет состоять из одного скрытого слоя\n",
    "layers = []\n",
    "layers.append(FCLayer(train_images.shape[1], 100)) # входной слой\n",
    "layers.append(ReLU()) # активация\n",
    "layers.append(FCLayer(100, 200)) # скрытый слой\n",
    "layers.append(ReLU()) # активация\n",
    "layers.append(FCLayer(200, 10)) # выходной слой -- 10 классов (10 цифр). Обратите внимание, мы не используем активацию!\n",
    "                                # т.к. лосс рассчитывает на логиты, а не вероятности\n",
    "\n",
    "# инициализируем наш класс указанными слоями\n",
    "net = Network(layers=layers)\n",
    "# если все реализовано правильно -- точность на отложенной части выборки достигнет 97%\n",
    "net.fit(x_train=train_images, y_train=train_labels, \n",
    "        x_test=test_images, y_test=test_labels,\n",
    "        batch_size=32, num_epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfOs96iPfJCj"
   },
   "outputs": [],
   "source": [
    "predicted_labels = net.predict(test_images)\n",
    "idxs = np.random.choice(np.arange(len(test_images)), 16, replace=False)\n",
    "show_mnist(test_images[idxs].reshape((-1, 28, 28)), test_labels[idxs], predicted_labels[idxs])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
